# Model architecture parameters
_target_: "EEGTransformer"
F1: 128                             # number of temporal filters
D: 2                                # spatial filter multiplier
eegnet_kernel_size: 64              # temporal convolution kernel length
eegnet_separable_kernel_size: 16    # separable convolution kernel length
eegnet_pooling_1: 8                 # first pooling window
eegnet_pooling_2: 4                 # second pooling window
dropout_eegnet: 0.3                 # dropout probability in EEGNet
# Number of heads needs to be divible by sequence_length_transformer = sequence_length//eegnet_pooling_1//eegnet_pooling_2
MSA_num_heads: 3                   # multi-head self-attention heads
transformer_dim_feedforward: 2048   # feedforward dimension in transformer layers
num_transformer_layers: 6           # number of stacked transformer encoder layers
flag_positional_encoding: true      # enable positional encoding
num_eeg_channels: 19
sequence_length: 3000               # time_window * resampleFS = 12 * 250

optimizer:
  lr: 1e-4

scheduler:
  step_size: 10
  gamma: 0.1
