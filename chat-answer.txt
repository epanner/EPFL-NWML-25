# project/
# ├── config/
# │   ├── config.yaml
# │   ├── model/
# │   │   └── gnn.yaml
# │   └── preprocessing/
# │       └── fft_filter.yaml
# ├── main.py
# └── requirements.txt

# ----- requirements.txt -----
numpy
pandas
scipy
torch
torch-geometric
pytorch-lightning
hydra-core
wandb

# ----- config/config.yaml -----
defaults:
  - preprocessing: fft_filter
  - model: gnn
  - _self_

batch_size: 128
num_epochs: 50
seed: 42
learning_rate: 1e-3

# ----- config/preprocessing/fft_filter.yaml -----
filter_order: 4
low_freq: 0.5
high_freq: 30
fs: 250

# ----- config/model/gnn.yaml -----
hidden_channels: 64
num_layers: 3
dropout: 0.2

# ----- main.py -----
import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv\import pytorch_lightning as pl
from torch.utils.data import DataLoader
import numpy as np
import wandb
import hydra
from omegaconf import DictConfig
from seiz_eeg.dataset import EEGDataset
from scipy import signal

class EEGGNN(pl.LightningModule):
    def __init__(self, cfg):
        super().__init__()
        self.save_hyperparameters()
        self.convs = torch.nn.ModuleList()
        self.convs.append(GCNConv(19, cfg.model.hidden_channels))
        for _ in range(cfg.model.num_layers - 1):
            self.convs.append(GCNConv(cfg.model.hidden_channels, cfg.model.hidden_channels))
        self.fc = torch.nn.Linear(cfg.model.hidden_channels, 1)
        self.dropout = cfg.model.dropout
        self.lr = cfg.learning_rate

    def forward(self, x, edge_index):
        for conv in self.convs:
            x = conv(x, edge_index).relu()
            x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.fc(x.mean(dim=0))  # Pooling (mean)
        return x

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x.x, x.edge_index)
        loss = F.binary_cross_entropy_with_logits(logits.squeeze(), y.float())
        self.log('train_loss', loss)
        return loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.lr)


def fft_filtering(x, cfg):
    bp_filter = signal.butter(cfg.filter_order, (cfg.low_freq, cfg.high_freq), 
                              btype="bandpass", output="sos", fs=cfg.fs)
    x = np.abs(np.fft.fft(signal.sosfiltfilt(bp_filter, x, axis=0), axis=0))
    x = np.log(np.where(x > 1e-8, x, 1e-8))
    win_len = x.shape[0]
    return x[int(cfg.low_freq * win_len // cfg.fs): int(cfg.high_freq * win_len // cfg.fs)]


@hydra.main(config_path="config", config_name="config")
def main(cfg: DictConfig):
    pl.seed_everything(cfg.seed)

    wandb.init(project="eeg-gnn", config=dict(cfg))

    dataset_tr = EEGDataset(
        clips_tr,
        signals_root=DATA_ROOT / "train",
        signal_transform=lambda x: fft_filtering(x, cfg.preprocessing),
        prefetch=True
    )

    loader_tr = DataLoader(dataset_tr, batch_size=cfg.batch_size, shuffle=True)

    model = EEGGNN(cfg)

    trainer = pl.Trainer(max_epochs=cfg.num_epochs, logger=pl.loggers.WandbLogger())
    trainer.fit(model, loader_tr)

    wandb.finish()


if __name__ == '__main__':
    main()
