_wandb:
    value:
        cli_version: 0.19.10
        m: []
        python_version: 3.11.11
        t:
            "1":
                - 1
                - 5
                - 41
                - 50
                - 53
                - 55
                - 77
                - 106
            "2":
                - 1
                - 5
                - 41
                - 50
                - 53
                - 55
                - 77
                - 106
            "3":
                - 13
                - 16
                - 23
                - 55
            "4": 3.11.11
            "5": 0.19.10
            "8":
                - 5
            "10":
                - 20
            "12": 0.19.10
            "13": linux-x86_64
cfg:
    value: '{''F1'': 32, ''D'': 2, ''eegnet_kernel_size'': 32, ''eegnet_separable_kernel_size'': 16, ''eegnet_pooling_1'': 8, ''eegnet_pooling_2'': 4, ''dropout_eegnet'': 0.3, ''MSA_num_heads'': 3, ''transformer_dim_feedforward'': 2048, ''num_transformer_layers'': 6, ''flag_positional_encoding'': True, ''num_eeg_channels'': 19, ''sequence_length'': 3000, ''optimizer'': {''lr'': 0.0001}, ''scheduler'': {''step_size'': 10, ''gamma'': 0.1}}'
checkpoint:
    value: '{''monitor'': ''val_loss'', ''monitor_comp_mode'': ''train_loss'', ''save_top_k'': 1}'
dataset:
    value: '{''data_path'': ''/home/veit/Uni/Lausanne/NML/EPFL-NWML-25/content/networkML'', ''train_set'': ''train/train'', ''test_set'': ''test/test'', ''distances_set'': ''distances_3d.csv''}'
do_not_create_model:
    value: true
early_stopping:
    value: '{''monitor'': ''val_loss'', ''monitor_comp_mode'': ''train_loss'', ''patience'': 50}'
model:
    value: '{''F1'': 32, ''D'': 2, ''eegnet_kernel_size'': 32, ''eegnet_separable_kernel_size'': 16, ''eegnet_pooling_1'': 8, ''eegnet_pooling_2'': 4, ''dropout_eegnet'': 0.3, ''MSA_num_heads'': 3, ''transformer_dim_feedforward'': 2048, ''num_transformer_layers'': 6, ''flag_positional_encoding'': True, ''num_eeg_channels'': 19, ''sequence_length'': 3000, ''optimizer'': {''lr'': 0.0001}, ''scheduler'': {''step_size'': 10, ''gamma'': 0.1}}'
preprocessing:
    value: '{''steps'': [{''_target_'': ''github_filter'', ''lowcut'': 0.5, ''highcut'': 50, ''fs'': 250, ''resampleFS'': 250}]}'
train:
    value: '{''max_epochs'': 1000, ''batch_size'': 512, ''seed'': 42, ''prefetch_dataset'': False, ''gradient_clip_val'': 0.5, ''comp_mode'': False}'
